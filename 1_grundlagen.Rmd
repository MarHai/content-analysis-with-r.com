---
title: "Automatisierte Inhaltsanalyse mit R"
author: "Cornelius Puschmann"
subtitle: "Grundlagen"
output: html_notebook
---

<!---
* Zeit-SML: Titel durch Volltexte ersetzen?
* MdB-SML: Split in Traings- und Testset?
* 
-->

\
\
\

[inhaltsanalyse-mit-r.de](http://inhaltsanalyse-mit-r.de/)

Dieses erste Kapitel liefert einen Überblick über zahlreiche Funktionen des Pakets [quanteda](https://quanteda.io/), die gleichzeitig die Grundlage der automatisierten Inhaltsanalyse mit [R](https://www.r-project.org/) bilden. Über quanteda hinaus werden im Verlauf dieser neunteiligen Einführung noch eine Reihe weiterer R-Bibliotheken verwendet, etwa für das Berechnen von [Themenmodellen](5_themenmodelle.html) (Kapitel 5) und das [überwachte maschinelle Lernen](6_maschinelles_lernen.html) (Kapitel 6). In praktisch jeder Einheit relevant sind dabei die Pakete des [tidyverse](https://www.tidyverse.org/) (vor allem ggplot, dplyr und stringr), durch die zahlreiche Funktionen wie Plotten, Textverarbeitung und Datenmanagement gegenüber den R-Basisfunktionen stark verbessert werden. Pakete für einzelne Teilbereiche, die erst später eine Rolle spielen werden, sind u.a. [topicmodels](https://cran.r-project.org/package=topicmodels) und [stm](https://www.structuraltopicmodel.com/) (Themenmodelle), [RTextTools](http://www.rtexttools.com/) (überwachtes maschinelles Lernen), und [spacyr](https://github.com/quanteda/spacyr) (POS-Tagging und Named-Entity-Erkennung).

Die Basis der Analyse in diesem ersten Kapitel sind die beliebten Geschichten von Sherlock Holmes. Das Sherlock Holmes-Korpus besteht aus zwölf Erzählungen, die in dem 1892 erschienenem Band *The Adventures of Sherlock Holmes* zusammengefasst sind, und die man gemeinfrei unter anderem durch das [Internet Archive](https://archive.org/) herunterladen kann. Die für diese Einführung verwendete Fassung wurde zunächst dem Internet Archive entnommen und dann in zwölf Einzeldateien aufgeteilt. Natürlich können die vorgestellten Methoden auf die anderen hier behandelten Korpora angewandt werden -- das Beispiel dient nur dazu, sich langsam an quanteda und die Grundlagen der computergestützen Inhaltsanalyse zu gewöhnen.

**Sämtliche in dieser Einführung verwendeter Codebeispiele, Korpora und Lexika können [hier](inhaltsanalyse_mit_r.zip) heruntergeladen werden.**


#### Installation und Laden der benötigten R-Bibliotheken

Zunächst werden die notwendigen Bibliotheken installiert (sofern noch nicht vorhanden) und anschließend geladen. Zudem wird vorbereitend die Theme-Einstellung für das Paket ggplot gesetzt (dies sorgt für hübschere Plots). Diesen Schritt wiederholen wir zu Beginn jedes Kapitels, daher wird auf ihn später nicht mehr weiter eingegangen. In einigen Kapiteln werden noch weiteren Pakete gelanden, etwa für eine erweiterte Farbpalette ([RColorBrewer](https://cran.r-project.org/package=RColorBrewer)), Wortwolken ([wordcloud](https://cran.r-project.org/package=wordcloud)) oder um URLs zu parsen ([urltools](https://cran.r-project.org/package=urltools)).  

```{r Installation und Laden der benötigten R-Bibliotheken, message = FALSE}
if(!require("quanteda")) {install.packages("quanteda"); library("quanteda")}
if(!require("readtext")) {install.packages("readtext"); library("readtext")}
if(!require("tidyverse")) {install.packages("tidyverse"); library("tidyverse")}
if(!require("RColorBrewer")) {install.packages("RColorBrewer"); library("RColorBrewer")}
theme_set(theme_bw())
```


#### Einlesen der Daten und Anlegen eines Korpus

Nachdem alle notwendigen Pakete geladen wurden, können wir nun die Daten einlesen und daraus ein quanteda-Korpus erstellen. Für das Einlesen der Plaintext-Dateien wird die Funktion [readtext](https://www.rdocumentation.org/packages/readtext) aus dem gleichnamigen Paket verwendet, durch die sich eine Reihe von Dateiformaten erfolgreich importieren lassen (u.a. TXT, PDF und Word). Grundsätzlich sich Plaintext–Daten (i.d.R. mit der Endung ".txt") und Daten in Tabellenform (etwa im Format CSV oder auch als Excel–Datei) für readtext ohne größere Probleme lesbar, allerdings muss man beim Einlesen erklären, wie genau die einzelnen Datensätze von einander getrennt sind (bei Plaintext–Dateien wo nicht 1 Datei == 1 Text, was etwa bei Exporten aus Lexis Nexis der Fall sein kann), bzw. welche Felder die Primär– und welche Metadaten beinhalten (bei Tabellen). Eine gute Einführung zum Paket readtext findet sich [hier](https://cran.r-project.org/web/packages/readtext/vignettes/readtext_vignette.html).

In diesem Fall entspricht jede Datei einem Text, wodurch der Import sehr umkompliziert ausfällt. Wir entfernen die Endung ".txt" aus dem Dokumentnamen, um diese später in Plot–Beschriftungen verwenden zu können. Schließlich wird die Variable *korpus* aufgerufen, was uns die wichtigen Eckdaten Dokumentanzahl und Docvars (Metadaten zu den Texten im Korpus) zurückliefert. 

```{r Daten einlesen und Korpus anlegen}
daten.sherlock <- readtext("daten/sherlock/romane/[0-9]*.txt") # Dateiname beginnt mit Zahl und endet mit .txt
daten.sherlock$doc_id <- str_sub(daten.sherlock$doc_id, start = 4, end = -5) # Dateiendung weglassen
korpus <- corpus(daten.sherlock, docid_field = "doc_id") # Korpus anlegen
docvars(korpus, "Textnummer") <- sprintf("%02d", 1:ndoc(korpus)) # Variable Textnummer generieren
korpus
```

In den folgenden Abschnitten werden häufig bereits vorbereitete Korpora geladen, d.h. der Befehl *corpus* wird hier nicht mehr explizit ausgeführt. Er ist aber im Vorfeld ausgeführt worden, um aus Textdatein auf der Festplatte oder Twitter-Daten in einem R-Data Frame ein quanteda-Korpus zu erstellen. 

Die Funktionen [ndoc](http://docs.quanteda.io/reference/ndoc.html), [ntoken](http://docs.quanteda.io/reference/ntoken.html), [ntype](http://docs.quanteda.io/reference/ntoken.html) und [nsentence](http://docs.quanteda.io/reference/nsentence.html) geben die Anzahl der Dokumente, Tokens, Types und Sätze aus. Diese Statistiken können bequem gemeinsam mit Metadaten auf Dokumentebene durch die Funktion [summary](https://www.rdocumentation.org/packages/quanteda/versions/1.3.0/topics/summary.corpus) erstellt werden. Bei den meisten Korpora, die hier verwendet werden, liegt ein solcher Data Frame mit Statistiken zu jedem Text bereits bei. Notwendig ist dies allerdings nicht. Will man auf Korpus–Metadaten zurückgreifen oder diese verändern, kann man dies jederzeit über den Befehl [docvars](http://docs.quanteda.io/reference/docvars.html) tun.

```{r Korpusstatistiken berechnen}
korpus.stats <- summary(korpus, n = 1000000)
korpus.stats$Text <- reorder(korpus.stats$Text, 1:ndoc(korpus), order = T)
korpus.stats
```

Das Funktionsargument n = 1000000 wird hier nur deshalb verwendet, weil die Funktion *summary* ansonsten nur maximal 100 Texte zusammenfasst. In diesem Fall reicht das zwar aus, aber bei größeren Datensätzen ist das eher unpraktisch. Technisch gesehen heißt diese Funktion *summary.corpus* und ist eine an Korpus-Objekte angepasste Variante der Basisfunktion *summary*, die auch sonst in R verwendet wird. Der Befehl [reorder](https://www.rdocumentation.org/packages/stats/versions/3.3/topics/reorder.default) wird verwendet, um die Texte nach ihrer Reihenfolge in *The Adentures of Sherlock Holmes* zu sortieren, statt alphabetisch nach Titel.


#### Basisstatistiken zu einem Korpus berechnen

Der Inhalt der Variable *korpus.stats* kann natürlich auch geplottet werden, um einen anschaulichen Eindruck von der Korpusbeschaffenheit zu geben. Die folgenden Zeilen liefern die Anzahl der Tokens (laufende Wörter), die Anzahl der Types (einmalige Wörter), und Sätze pro Roman zurück (vgl. dazu [diese Einführung](https://www.bubenhofer.com/korpuslinguistik/kurs/index.php?id=erstellung_korpora.html)). Schließlich wird noch das Verhältnis von Typen zu Tokens (oder die sog. [Typ-Token-Relation](https://de.wikipedia.org/wiki/Type-Token-Relation)) geplottet. 

Grundlage solcher Plots sind praktisch immer Data Frame-Objekte (also Tabellen), die Informationen über Korpora, Texte, Wörter, Themen usw. enthalten, welche sich visuell darstellen lassen. Im Rest dieser Einführung gehe ich nicht im Detail darauf ein, wie die jeweiligen Plots genau konstruiert werden, allerdings lassen sich die meisten Daten auch (etwas weniger ansprechend) mit der R-internen Funktion [plot()](https://www.rdocumentation.org/packages/graphics/versions/3.5.0/topics/plot) darstellen. Eine hilfreiche deutschsprachige Einführung in das Plotten mit ggplot2 findet sich [hier](http://md.psych.bio.uni-goettingen.de/mv/unit/ggplot2/ggplot2.html). Viele der hier vorgestellten Plots stammen zudem direkt aus quanteda (beginnend mit textplot_).

```{r Tokens, Types, Sätze und TTR pro Roman plotten}
ggplot(korpus.stats, aes(Text, Tokens, group = 1)) + geom_line() + geom_point() + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + ggtitle("Tokens pro Roman")
ggplot(korpus.stats, aes(Text, Types, group = 1)) + geom_line() + geom_point() + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + ggtitle("Types pro Roman")
ggplot(korpus.stats, aes(Text, Sentences, group = 1)) + geom_line() + geom_point() + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + ggtitle("Sätze pro Roman")
ggplot(korpus.stats, aes(Tokens, Types, group = 1, label = Textnummer)) + geom_smooth(method = "lm", se = FALSE) + geom_text(check_overlap = T) + ggtitle("Typ-Token-Relation pro Roman")
```

Diese Grafiken sind zunächst einmal nicht umwerfend informativ. Sie belegen lediglich, dass die Erzählungen ‘A Case of Identity’ und (in geringerem Maße) ‘The Five Orangen Pips’ deutlich kürzer sind als die anderen Texte, was sich auf allen drei Ebenen (Tokens, Types, Sätze) niederschlägt. Etwas interessanter wird es allerdings bei der Typ-Token-Relation: während drei Romane (mit den Nummern 3, 11 und 12) jeweils einen eher unterdurchschnittlichen TTR aufweisen, liegen weitere vier oberhalb der linearen Relation (1, 5, 6  und 8), während die verbleibenden sechs ziemlich genau dem Durchschnitt entsprechen. Über den TTR lassen sich Rückschlüssen über die Informationsdichte ziehen -- dazu später noch mehr. 


#### Mit Korpora arbeiten

Korpora lassen sich in quanteda sehr leicht samplen, umformen und mit zusätzlichen Metadaten versehen. Metadaten können wiederum genutzt werden, um das Korpus nach bestimmten Kriterien zu filtern. Der folgenden Aufruf zeit die erste 1.000 Wörter des ersten Romans.

```{r Substring extrahieren}
str_sub(korpus[1], start = 1, end = 1000) # Anfang des ersten Romans wiedergeben
```

Jeder Text lässt sich also anhand seiner Indizierung aufrufen und auch ändern (etwa korpus[1] für den ersten Text). Gleiches funktioniert auch über die Funktion [texts](http://docs.quanteda.io/reference/texts.html) –– der Weg über die den Index ist lediglich die Kurzform von texts(korpus)[1]. 

Mittels [corpus_reshape](http://docs.quanteda.io/reference/corpus_reshape.html) lässt sich ein Korpus so umformen, dass jeder Satz ein eigenes Dokument ergibt. Alternative Argumente sind "paragraphs" und "documents" (so lässt sich ein Satz-Korpus wieder in seinen Anfangszustand zurückversetzen). Die Erstellung von Satz-Korpora ist für die Sentimentanalyse und das überwachte maschinelle Lernen von Interesse. 

Die Beschriftung des Beispiels besteht hier aus der Variable *docname* und einer angehängten Zahl (eine 1 für den ersten Satz). 

```{r Korpus zu Sätzen umformen}
korpus.saetze <- corpus_reshape(korpus, to = "sentences")
korpus.saetze[1]
```

Mit [corpus_sample()](http://docs.quanteda.io/reference/corpus_sample.html) kann weiterhin ein zufälliges Sample aus einem Korpus gezogen werden. Wir wenden die Funktion hier auf das Satz-Korpus an. 

```{r Zufallsample ziehen}
zufallssatz <- corpus_sample(korpus.saetze, size = 1)
zufallssatz[1]
```

Anhand von [corpus_subset](http://docs.quanteda.io/reference/corpus_subset.html) kann ein Korpus schließlich nach Metadaten gefiltert werden. Hier geschieht dies mittels der neu erstellten binären Variable *LangerSatz*, die dann TRUE ist, wenn ein Satz >= 25 Tokens enthält). So lässt sich ein Teilkorpus zu bilden, in dem nur längere Sätze enthalten sind. Das Beispiel soll lediglich verdeutlichen, dass mithilfe der von quanteda bereitgestellten Funktionen zahlreiche Schritte für die Bereinigung von Korpora möglich sind. 

```{r Neue Docvars hinzufügen}
docvars(korpus.saetze, "Zeichenanzahl") <- ntoken(korpus.saetze)
docvars(korpus.saetze, "LangerSatz") <- ntoken(korpus.saetze)>=25
korpus.saetze_lang <- corpus_subset(korpus.saetze, LangerSatz == TRUE)
korpus.saetze_lang[1:3]
```

Schließlich lassen sich Korpora mithilfe von [corpus_segment()](http://docs.quanteda.io/reference/corpus_segment.html) auch nach bestimmten Kriterien aufspalten.


#### Tokenisierung 

Unter Tokensierung versteht man die Aufspaltung eines Textes in laufende Wörter oder sog. N-Gramme, also Sequenzen mehrerer Wörter in Folge. Die Funktion [tokens](https://docs.quanteda.io/reference/tokens.html) realisiert die Tokenisierung eines Korpus in quanteda. Zusätzlich versteht *tokens* auch unzählige Argumente für die Entfernung bestimmter Features.

```{r Einfache Tokenisierung}
meine.tokens <- tokens(korpus)
head(meine.tokens$`A Scandal in Bohemia`)
```

Mittels der Funktion tokens lässt sich der Text über das Argument *ngrams* auch gleich in N-Gramme (Mehrwortsequenzen) aufspalten. Im folgenden Beispiel werden erst Bigramme vom Anfang des ersten Textes angezeigt, und dann alle Sequenzen von einem, zwei oder drei Begriffen extrahiert (durch die Anwendung von [head](https://www.rdocumentation.org/packages/utils/versions/3.5.1/topics/head) sehen wir nur Trigramme, es sind aber auch kürzere Sequenzen vorhanden). 

```{r N-Gramme extrahieren}
meine.tokens <- tokens(korpus, ngrams = 2)
head(meine.tokens$`A Scandal in Bohemia`)

meine.tokens <- tokens(korpus, ngrams = 1:3)
head(meine.tokens$`A Scandal in Bohemia`)
```

Hilfreich ist auch die Möglichkeit, bei der Tokenisierung bestimmte Begriffe zu entfernen oder zurückzubehalten. 

```{r Tokens entfernen oder behalten}
meine.tokens <- tokens(korpus)
begriffe.behalten <- tokens_select(meine.tokens, c("holmes", "watson")) # Platzhalter mit padding = TRUE
head(begriffe.behalten$`A Scandal in Bohemia`)
begriffe.entfernen <- tokens_remove(meine.tokens, c("Sherlock", "in", "is", "the"))
head(begriffe.entfernen$`A Scandal in Bohemia`)
```

Wie bereits angedeutet akzeptiert die Funktion *tokens* eine Reihe von Argumenten, mit denen ganze Klassen von Zeichenketten (Zahlen, Interpunktion, Symbole usw.) ausgeschlossen oder zurückbehalten werden können. Folgend werden zunächst Zahlen, Interpunktion und Symbole entfernt, dann mittels [tokens_tolower](https://docs.quanteda.io/reference/tokens_tolower.html) alle Wörter in Kleinschreibung umgewandelt und dann dann noch die Wörter "sherlock" und "holmes", sowie eine Reihe englischer [Stoppwörter](https://de.wikipedia.org/wiki/Stoppwort) entfernt. 

```{r weitere Argumente}
meine.tokens <- tokens(korpus, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE)
meine.tokens <- tokens_tolower(meine.tokens)
meine.tokens <- tokens_remove(meine.tokens, c(stopwords("english"), "sherlock", "holmes"))
head(meine.tokens$`A Scandal in Bohemia`)
```

Das Resultat ist der Art von Daten, mit denen man bei Verfahren wie der Anwendung von Lexika (Kapitel 2), der Berechnung von Themenmodellen (Kapitel 3), und dem überwachten maschinellen Lernen (Kapitel 4) häufig arbeitet sehr ähnlich. Durch die Stoppwortentfernung und andere Schritte gehen syntaktische Informationen verloren, d.h. man kann nicht mehr nachvollziehen, wer was mit wem tut, oder wie der Text insgesamt argumentativ oder erzählerisch aufgebaut ist. Diese Informationen sind allerdings im "[Bag-of-Words-Ansatz](https://en.wikipedia.org/wiki/Bag-of-words_model)", der in der automatisierten Inhaltsanalyse nahezu immer verwendet wird, nicht unbedingt relevant.

Die in diesem Abschnitt beschriebenen Schritte sind zwar im Einzelfall nützlich, werden aber in den folgenden Kapitel praktisch nicht angewandt, weil die Daten dort schon als quanteda–Korpora vorliegen, und weil zudem häufig auch bis auf die Anwendung der Funktion *corpus* keine weiteren Schritte notwendig sind.


#### Dokument-Feature-Matrizen (DFMs) erstellen

Wir kommen nun zu einer zentralen Datenstruktur von quanteda, die im Gegensatz zu den zuvor vorgestellten Einheiten praktisch in jedem Projekt vorkommt: die Document Feature-Matrize (DFM). Üblicherweise wird direkt nachdem ein Korpus angelegt wurde eine DFM berechnet, zuweilen auch mehrere. Eine DFM ist eine Tabelle, deren Zeilen Texte und deren Spalten Wortfrequenzen enhalten. Dabei gehen Informationen darüber, wo in einem Text ein Wort vorkommt verloren (man spricht auch vom '[Bag-of-Words-Ansatz](https://en.wikipedia.org/wiki/Bag-of-words_model)'). Immer dann, wenn wir uns für die Beziehung von Wörtern zu Texten (und umgekehrt) interessieren, berechnen wir eine DFM.

```{r einfache DFM erstellen}
meine.dfm <- dfm(korpus, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove = stopwords("english"))
meine.dfm
```

Wichtig: Hier wird implizit der uns schon vertraute Befehl [tokens()](https://docs.quanteda.io/reference/tokens.html) angewandt, um bestimmte Features zu entfernen. Vieles funktioniert bei DFMs analog zur Erstellung eines Korpus. So zählen die Funktionen [ndoc()](https://docs.quanteda.io/reference/ndoc.html) und [nfeat()](https://docs.quanteda.io/reference/ndoc.html) Dokumente und Features (Wörter).

```{r Dokumente und Features zählen}
ndoc(meine.dfm)
nfeat(meine.dfm)
```

Mittels der Funktionen [docnames()](https://docs.quanteda.io/reference/docnames.html) und 
[featnames()](https://docs.quanteda.io/reference/featnames.html) lassen sich die Namen der Dokumente und Features ausgeben.

```{r Dokumente und Features anzeigen}
head(docnames(meine.dfm)) # In der DFM enthaltene Dokumente 
head(featnames(meine.dfm), 50) # Features in chronologischer Reihenfolge
```

Die tabellarische Ansicht illustriert den Inhalt der DFM als Text-Wort-Matrix am besten. Die sparsity ("Spärlichkeit") einer DFM beschreibt dabei den Anteil der leeren Zellen, also Wörter, die nur in sehr wenigen Texten vorkommen. Wie sich leicht ableiten lässt, werden DFMs sehr schnell sehr groß. Zum Glück macht sich quanteda eine Reihe von für den Nutzer unsichtbaren Funktionen aus anderen Paketen zunutze, um diesem Problem zu begegnen. 

```{r Features/Texte als Matrix}
head(meine.dfm, n = 12, nf = 10) # Features/Texte als Matrix
```

Gleich an den ersten Blick fällt auf, das die Wörter 'sherlock' und 'holmes' in allen Romanen vorkommen, also sehr wenig distinktiv sind, weshalb wir sie unter Umständen zu den Stoppwörtern für dieses Korpus hinzufügen sollten. 

Die Funktion [topfeatures()](https://docs.quanteda.io/reference/topfeatures.html) zählt Features in der gesamten DFM aus. Die Funktion [textstat_frequency()](https://docs.quanteda.io/reference/textstat_frequency.html) liefert zusätzlich noch den Rang (rank), die Anzahl der Dokumente, in denen das Feature vorkommt (docfreq) sowie Metadaten, nach denen bei der Zählung gefiltert wurde.

```{r Worthäufigkeiten berechnen}
topfeatures(meine.dfm) # Features nach Frequenz
worthaeufigkeiten <- textstat_frequency(meine.dfm) # Worthäufigkeiten
head(worthaeufigkeiten)
```


#### Mit DFMs arbeiten

DFMs lassen sich mit [dfm_sort](https://docs.quanteda.io/reference/dfm_sort.html) leicht nach Dokument- und Feature-Frequenzen sortieren.

```{r DFMs sortieren}
head(dfm_sort(meine.dfm, decreasing = TRUE, margin = "both"), n = 12, nf = 10) 
```

Weiterhin lassen sich bestimmte Features einer DFM gezielt mittels [dfm_select](https://docs.quanteda.io/reference/dfm_select.html) auswählen.

```{r DFMs filtern}
dfm_select(meine.dfm, pattern = "lov*")
```

Die Funktion [dfm_wordstem()](https://docs.quanteda.io/reference/dfm_wordstem.html) reduziert Wörter auf ihre Stammform. Diese Funktion existiert in quanteda derzeit nur für Englisch und ist auch dort nur begrenzt zuverlässig, was die folgende Ausgabe gut illustriert ('holm' ist kein Wortstamm). 

```{r Wortstammreduktion}
meine.dfm.stemmed <- dfm_wordstem(meine.dfm)
topfeatures(meine.dfm.stemmed)
```

Ebenso wie bei Wortfrequenzen in Korpora ist die Gewichtung einer DFM nach relativen Wortfrequenzen und Verfahren wie [TF-IDF](https://de.wikipedia.org/wiki/Tf-idf-Ma%C3%9F) oftmals sinnvoll. Die Gewichtung einer DFM funktioniert immer aufgrund der Wort-Text-Relation, weshalb topfeatures() in Kombination mit [dfm_weight()](https://docs.quanteda.io/reference/dfm_weight.html) merkwürdige Resultate produziert. Relative Frequenzen und TF-IDF sind nur kontrastiv innerhalb der Text in einem Korpus sinnvoll (hier für 'A Scandal in Bohemia'), da für das gesamte Korpus relative Frequenz == absolute Frequenz

```{r DFMs gewichten}
meine.dfm.proportional <- dfm_weight(meine.dfm, scheme = "prop")
topfeatures(meine.dfm) # absolute Frequenzen für das gesamte Korpus
topfeatures(meine.dfm.proportional) # ...ergibt wenig Sinn
topfeatures(meine.dfm.proportional[1,]) # ...ergibt mehr Sinn
```

Im zweiten Beispiel sehen wir etwa, dass 'A Scandal in Bohemia' einen leicht höheren Anteil von Nennungen der Wortes 'holmes' hat, als dies im Gesamtkorpus der Fall ist. Dazu später noch etwas mehr.  

Die Gewichtungsansätze Propmax und TF-IDF liefern relevante Wortmetriken, zum Beispiel für die Bestimmung von Stoppwörtern. Propmax skaliert die Worthäufigkeit relativ zum frequentesten Wort (hier 'holmes'). Funktional ähneln sich TF-IDF und der später vorgestellte Keyness-Ansatz -- beide finden besonders distinktive Terme.

```{r Propmax-Gewichtung und TF-IDF}
meine.dfm.propmax <- dfm_weight(meine.dfm, scheme = "propmax")
topfeatures(meine.dfm.propmax[1,])

meine.dfm.tfidf <- dfm_tfidf(meine.dfm)
topfeatures(meine.dfm.tfidf)
```

Schließlich lässt sich mit [dfm_trim()](https://docs.quanteda.io/reference/dfm_trim.html) noch eine reduzierten Dokument-Feature-Matrix erstellen. Das ist dann sinnvoll, wenn man davon ausgeht, dass beispielsweise nur solche Begriffe eine Rolle spielen, die mindestes X mal im Gesamtkorpus vorkommen. Auch eine Mindestzahl oder ein Maximum an Dokumenten, in denen ein Begriff vorkommen muss oder darf, kann bestimmt werden. Schließlich lassen sich beide Filteroptionen auch proportional anwenden (vgl. Beispiel).

Features, die mindestens in 11 Romanen vorkommen
Features im 95. Häufigkeitsperzentil (=Top 5% aller Features)

```{r DFMs trimmen}
meine.dfm.trim <- dfm_trim(meine.dfm, min_docfreq = 11)
head(meine.dfm.trim, n = 12, nf = 10) 

meine.dfm.trim <- dfm_trim(meine.dfm, min_termfreq = 0.95, termfreq_type = "quantile")
head(meine.dfm.trim, n = 12, nf = 10) 
```


#### DFMs visualisieren

DFMs lassen sich u.a. auch als Wortwolke der häufigsten Begriffe darstellen. 

```{r Wortwolke nach Häufigkeit}
textplot_wordcloud(meine.dfm, max_words = 100, scale = c(5,1))
```

Interessanter als die Darstellung des Gesamtkorpus ist auch hier der Vergleich. Das folgende Plot zeigt die distinktivsten Begriffe nach TF-IDF für vier Romane, wobei die Farbe den jeweiligen Roman kennzeichnet. Dass im Plot die Wortgröße nicht die absolute Frequenz anzeigt, sondern den TF-IDF-Wert, macht ein solches Plot für den unmittelbaren Vergleich nützlich.

```{r Wortwolke nach TF-IDF je Roman}
textplot_wordcloud(meine.dfm.tfidf[1:4,], color = brewer.pal(4, "Set1"), comparison = T)
```


