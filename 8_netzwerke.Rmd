---
title: "Automatisierte Inhaltsanalyse mit R"
author: "Cornelius Puschmann"
subtitle: "Texte und Wörter als Netzwerke"
output: html_notebook
---

<!---
* Begriffs-Netzwerk MdBs mit Partei als Farbe
-->

[inhaltsanalyse-mit-r.de](http://inhaltsanalyse-mit-r.de/)

Texte und Wörter lassen sich neben den bisher vorgestellen Techniken auch noch mit einem weiteren Ansatz untersuchen. Die Netzwerkanalyse stellt eine sehr wichtige Methode der Sozialwissenschaften dar, die grundsätzlich für die Untersuchung von Akteursbeziehungen von großer Relevanz ist. Sie ist aber darüber hinaus auch durchaus für die automatisierte Inhaltsanalyse von Bedeutung, zum einen, weil sich mit ihr etwa abbilden lässt, welche Akteure zusammen in einem Text genannte werden, aber auch dann, wenn beispielsweise ermittelt werden soll, wer ähnliche Begriffe verwendet.

Bereits im zweiten Kapitel wurde eine der wichtigsten Komponenten von quanteda eingeführt: die DFM. In dieem Kapitel wenden wir uns nun ihrem pendant z, der fcm.

```{r Packetinstallation, message = FALSE}
# Installation und Laden der Bibliotheken
if(!require("quanteda")) {install.packages("quanteda"); library("quanteda")}
if(!require("tidyverse")) {install.packages("tidyverse"); library("tidyverse")}
if(!require("devtools")) {install.packages("devtools"); devtools::install_github("cbail/textnets")}
if(!require("textnets")) {devtools::install_github("cbail/textnets"); library("textnets")}
theme_set(theme_bw())
```

Nachdem wir die notwendigen Bibliotheken gelanden haben, greifen wir eneut auf die Variante des Sherlock Holmes-Korpus zurück, welche die zwölf Romane in 174 Abschnitte gleicher Länge unterteilt. Wir erstellen eine DFM auf Basis von Bigrammen (Zweiwort-Folgen) in der wir die Unterscheidung zwischen Groß- und Kleinschreibung aufrechterhalten. 

```{r Laden der Daten und Berechnen einer DFM}
# Laden der Sherlock Holmes-Daten (bereits als RData-File gespeichert)
load("daten/sherlock/sherlock.absaetze.RData")

# Berechnen einer DFM
meine.dfm <- dfm(korpus, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, tolower = F, ngrams = 2)
```

Dann Laden wir eine einfache Liste von Personen, welche in den Romanen vorkommen. Durch unterschiedliche Varianten des gleichen Namens (Sherlock Holmes vs. Mr. Holmes vs. Sherlock) ist dies Methode nicht ganz akkurat, es genügt aber für eine einfache Demonstration.  

```{r Filtern der DFM nach Personennamen}
personen <- scan("daten/sherlock/sherlock.personen.txt", what = "char", sep = "\n")
meine.dfm.personen <- dfm_select(meine.dfm, personen, selection = "keep", case_insensitive = F)
```

Nun bereichnen wir eine Feature Cooccurance Matrix (FCM) welche auf der bereits vorhandenen DFM basiert. Dann plotten wir das Ergebnis als Netzwerk. 

```{r Berechnen einer FCM und Plotten eines Textnetzwerkes}
meine.fcm <- fcm(meine.dfm.personen)
textplot_network(meine.fcm)
```

Auch wenn man sich noch nicht eingehend mit Netzwerken beschäftigt hat, lässt sich das Ergebnis leicht intuitiv interpretieren. Zwei Namen ("Knoten") sind dann mit einer Linie verbunden wenn sie innerhalb des gleichen Dokuments (in diesem Fall also im gleichen Absatz) vorkommen. Verbindungslinien ("Kanten") sind dort dicker, wo zwei Namen mehrmals gemeinsam erwähnt werden. 

Die FCM ist die Grundlage des Netzwerkes. Während die Zeilen einer DFM die Dokumente und die Spalten die Wörter enthalten, enthält eine FCM Wörter als Zeilen und Spalten, und die Anzahl der Kookkurenzen als Zelleninhalt. Stimmen der Inhalt von Zeile und Spalte überein, wird einfach die Gesamtanzahl der Treffer im Korpus bezeichnet. 

```{r Darstellung der FCM als Tabelle}
convert(meine.fcm, to = "data.frame")
```

Was lässt sich aus der Kookkurenz von Begriffen in einem größeren Korpus ableiten? Wir laden das Bundestags-Korpus und extrahieren zentrale Begriffe nach Redner mittels TD-IDF, um diese Frage zu beanrtworten. 

```{r}
load("daten/bundestag/bundestag.korpus.RData")
korpus.bundestag <- corpus_subset(korpus.bundestag, type == "speech")
meine.dfm <-
  dfm(korpus.bundestag,
      groups = "speaker_key",
      remove_numbers = TRUE,
      remove_punct = TRUE,
      remove_symbols = TRUE,
      remove = stopwords("german")) %>%
  dfm_trim(min_termfreq = 7, max_termfreq = 2283) %>%
  dfm_tfidf(.)
textstat_frequency(meine.dfm, n = 300)
```

Wir verwenden die 300 nach TF-IDF distinktivsten Begriffe nun für eine einfaches Lexikon, mithilfe dessen wir ein Netzwerk erstellen, welches uns die Kookkurenz der Begriffe in Redebeiträgen zeigt.

```{r}
netzwerk.begriffe <- scan("daten/bundestag/netzwerk_begriffe.txt", what = "char", sep = "\n")
meine.dfm <- dfm(korpus.bundestag)
meine.dfm.netzwerk <- dfm_keep(meine.dfm, netzwerk.begriffe)
meine.fcm <- fcm(meine.dfm.netzwerk)
textplot_network(meine.fcm, edge_alpha = 0.2)
```

Wir wenden 

```{r}
meine.dfm <- dfm(korpus.bundestag, groups = "speaker_key")
meine.dfm.netzwerk <- dfm_keep(meine.dfm, netzwerk.begriffe)
nw <- convert(meine.dfm.netzwerk, to = "matrix")
ueb <- NULL
ueb.df <- data.frame(0, nrow = 1, ncol = ncol(nw))
for (i in 1:nrow(nw))
{
  for (j in 1:nrow(nw)) ueb <- c(ueb, cor(nw[i,], nw[j,]))
  ueb.df <- bind_rows(ueb.df, ueb)
}

head(x, 1)


korpus.bundestag.sample <- corpus_sample(korpus.bundestag, 1000)
bundestag.sample <- data.frame(text = texts(korpus.bundestag.sample), docvars(korpus.bundestag.sample), stringsAsFactors = F)
bundestag.tt <- PrepText(bundestag.sample, groupvar = "speaker_cleaned", textvar = "text", node_type = "groups", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)
```

