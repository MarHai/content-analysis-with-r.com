---
title: "R Notebook"
output: html_notebook
---

Der Import von Daten gehört immer zu den wichtigsten Themen, wenn es um studentische Projekte unter Nutzung der automatisierten Inhaltsanalyse geht. Aber auch bei großen Forschungvorhaben ist die Verfügbarkeit, Vorverarbeitung und Speicherung von Textdaten grundsätzlich ein sehr bedeutender Aspekt. Daher mag es verwundern, dass dieses Kapitel nicht gleich zu Beginn dieser Einführung steht, sondern am Schluss. Grund für diesem Umstand ist die Erfahrung, dass die erfolgreiche Datenbeschaffung und der Import von Daten oft (zu) viel Zeit in Anspruch nehmen, worunter die Analyse häufig leidet. Gerade in der Lehre ist dies aus offensichtlichen Gründen ein Problem. Aus diesem Grund gebe ich in dieser Einführung bewusst Daten vor, auch wenn das gewisse Einschränkungen bezüglich der Vielfalt der möglichen Projekte darstellt. 

Ein weiterer Gesichtspunkt, der mitunter vergessen wird, liegt in der stetig wachsenden Zahl exzellenter Korpora auf Plattformen wie [GESIS Datorium](https://datorium.gesis.org/) und dem [Harvard Dataverse](https://dataverse.harvard.edu/) begründet, die man nachnutzen kann, und sich sich über Werkzeuge wie die Google Datensatzsuche auffinden lassen. Das gilt auch dann, wenn Daten zwar theoretisch abrufbar sind, aber erst aufwändig vorverarbeitet werden müssen. Korpora wie das hier verwendete Bundestagskorpus lassen sich nur mit ausgesprochen hohem Aufwand selbst von den PDF–Dokumenten, als die sie auf den Seiten der Bundestagsverwaltung vorliegen, in einen geordneten Data Frame überführen. Da ist es intelligenter, standardisierte und gut dokumentierte Ressourcen zu verwenden, als das sprichwörtliche Rad neu zu erfinden.

Allerdings gibt es natürlich sehr viele Vorhaben, für die man eigens Daten erheben muss –– von Presseberichterstattung bis hin zu sozialen Medien. Folgend gebe ich eine Überblick über Ansätze, durch die dies möglich wird. Einerseits handelt es sich hierbei um Verfahren, die durch Programmierschnittstellen (APIs) realisiert werden und entsprechende R–Bibliotheken (Rfacebook, rtweet) nutzen, und andererseits um handgestrickte Skripte, die exportierte Daten aus Diensten wie Lexis Nexis oder von Webseiten einlesen (in einigen Fällen handelt es sich auch um Pakete, die genau dies vereinfachen sollen). Fehler sind zwar grundsätzlich immer möglich, im zweiten Modell schleichen sie sich allerdings besonders leicht ein, weil die exportierten Dateien oft Fehler oder Idiosynkratien aufweisen, die beim Einlesen nicht unbedingt sofort offenkundig sind. 

Für die folgenden Dienste stelle ich folgend Importstrategien vor:

Lexis Nexis
Factiva
COSMAS
Web of Science
MediaCloud
WikiNews
Twitter
Facebook
YouTube
Reddit
Web

Wie auch andere vergleichbare Dienste (Factiva, Web of Science) exportiert Lexis Nexis Datein, die sich in R einlesen lassen. Diese Dateien

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
plot(cars)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

